{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7403033e-9b40-453a-8d04-43744a332db5",
   "metadata": {},
   "source": [
    "# Assembling RADseq data with ipyrad\n",
    "\n",
    "\n",
    "\n",
    "Restriction site-Associated DNA sequencing (RADseq) is a high-throughput genotyping technique used in molecular biology and genomics. It is one method to generate reduced representation libraries, which allow us to prepare and sequence hundreds to thousands of genomic regions from across the genome without sequencing the entire genome. RADseq methods use restriction enzymes to cut up the genome and then sequence DNA regions that are adjacent to these cut sites. The idea is that within the same species or relatively closely related species, restriction enzyme cut sites should mostly be at the same places and allow for the selection of shared loci across samples without needing to develop sepecific probes.\n",
    "\n",
    "\n",
    "One of the most commonly used RADseq approaches, and the one that we'll use here, is double digest RADseq (ddRADseq) which uses two enzymes to cut up the genome and then a size selection step to further reduce the total set of total set of loci, which should ideally result in fewer loci that require less sequencing effort and that overlap among samples.\n",
    "\n",
    "We’ll be working with empirical double digest RADseq data [(Peterson et al. 2012)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0037135) that I (Sean Harrington) generated as part of my PhD research. The data are for a species of rattlesnake, the red diamond rattlesnake (*Crotalus ruber*), that is distributed across the Baja California peninsula and into southern California. I was interested in identifying if there is any population structure in *C. ruber* and inferring what population genetic and environmental forces have resulted in any existing structure. The data are single-end reads generated on an Illumina hiSeq. My analyses of these data are published in [Harrington et al. 2018](https://onlinelibrary.wiley.com/doi/full/10.1111/jbi.13114).\n",
    "\n",
    "\n",
    "\n",
    "The dataset is reasonably small and we should be able to quickly process and analyze it.\n",
    "\n",
    "We will use [ipyrad](https://ipyrad.readthedocs.io/en/master/) to process and assemble the raw data into alignments. ipyrad is a flexible python-based pipeline for taking various types of restriction-site associated data, processing them, and generated aligned datasets.\n",
    "\n",
    "ipyrad is capable of generating datasets either by mapping your raw reads to a reference genome or using a de novo assembly method that does not require a reference. We will use the de novo method here.\n",
    "\n",
    "If you need help with ipyrad outside of this workshop for specific issues, you can always post [here](https://app.gitter.im/#/room/#dereneaton_ipyrad:gitter.im). The developers are very responsive to queries.\n",
    "\n",
    "ipyrad is certainly not the only option for assembling RADseq data. [Stacks](https://catchenlab.life.illinois.edu/stacks/) and [dDocent](https://www.ddocent.com//) are other popular options, or there are various ways to manually assemble or map RADseq data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009069f0-bf36-4723-b83e-1f75699d6017",
   "metadata": {},
   "source": [
    "## Files and basic setup\n",
    "\n",
    "The files we will use are:\n",
    "\n",
    "- all_ruber.fastq\n",
    "- barcodes_samples.txt\n",
    "- names_ruber_all.txt\n",
    "\n",
    "We will copy these from the google bucket for this tutorial into your GCP instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ee98a-a4a8-4ec6-97de-c3468a53a164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil -m cp -r gs://radseq_cloud/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990df80-df0a-4e4a-a738-4e2ea53c68a6",
   "metadata": {},
   "source": [
    "## fastq format\n",
    "\n",
    "Before we start doing anything with the data, it's worth seeing what the raw data look like. The standard format for raw data for genomic sequences is fastq.\n",
    "\n",
    "Let's take a look at the first 8 lines of the fastq file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784010f-23fc-4c4a-ad8e-ef83163a90df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!zcat radseq_cloud/ruber_data/all_ruber.fastq.gz | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4b9cf6-c9e6-4128-8a1b-205cc9ee5113",
   "metadata": {},
   "source": [
    "- note that these reads are gzipped (compressed; end in .gz): you cannot directly look at them with `head` but instead need to use `zcat`, which reads gzipped files, and pipe the output to `head`. Fastq files are typically gzipped to save disk space, and most genomics programs can read gzipped fastqs.\n",
    "\n",
    "Each read from the sequencer is represented by 4 lines: the first 4 lines are the first read, the second set of 4 lines are the second read, etc. For each read, the first line is the header, and always starts with `@`. This contains a sequence identifier and various information about the read, often including information about the sequencing run. The second line, after the header, is the actual DNA sequence of the read. The next line always starts with `+` and may contain either no additional text, or the sequence identifier and extra information, as in the header. Line 4 for each read, following the `+` line, indicates the quality score for each DNA base in the read. This line will be exactly the same length as the DNA sequence in the second line, with e.g., the 4th character in this line corresponding to the quality of the 4th base in the sequence, etc.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Installing ipyrad\n",
    "\n",
    "Next up, we'll use mamba and pip to create a conda environment and install ipyrad into your instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf95e68a-125b-42cf-bb9d-3a019ff3840e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 74.3M  100 74.3M    0     0   170M      0 --:--:-- --:--:-- --:--:--  170M\n",
      "mamba 1.5.12\n",
      "conda 24.11.3\n",
      "\n",
      "Looking for: ['ipyrad']\n",
      "\n",
      "conda-forge/linux-64                                        Using cache\n",
      "conda-forge/noarch                                          Using cache\n",
      "bioconda/linux-64                                           Using cache\n",
      "bioconda/noarch                                             Using cache\n",
      "\n",
      "Pinned packages:\n",
      "  - python 3.10.*\n",
      "\n",
      "\n",
      "Transaction\n",
      "\n",
      "  Prefix: /opt/conda\n",
      "\n",
      "  All requested packages already installed\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Download the latest Mambaforge installer\n",
    "! curl -fL -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\n",
    "\n",
    "# Install Mambaforge without output\n",
    "! bash Miniforge3-$(uname)-$(uname -m).sh -b -p $HOME/miniforge3 > /dev/null 2>&1\n",
    "\n",
    "\n",
    "# Add the Miniforge bin directory to the system PATH environment variable\n",
    "import os\n",
    "os.environ[\"PATH\"] = os.environ[\"HOME\"] + \"/miniforge3/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Verify that conda is available in the PATH\n",
    "! mamba --version\n",
    "\n",
    "# Use mamba to install ipyrad\n",
    "! mamba install ipyrad -c conda-forge -c bioconda -y\n",
    "\n",
    "# Use pip to isntall the necessary psutil\n",
    "# ! pip install psutil==6.1.1\n",
    "\n",
    "# handle some compatibility with psutil\n",
    "! rm -f /opt/conda/lib/python3.10/site-packages/psutil/_psutil_linux.cpython-310-x86_64-linux-gnu.so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428fbb7-8628-4fee-8434-41c2f949fc5f",
   "metadata": {},
   "source": [
    "## Running iPyRad\n",
    "\n",
    "\n",
    "First, we need to generate a params file that contains the parameters we need to specify for ipyrad. In your scripts directory, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d27bc2-55db-474f-97e4-25782a6a836c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipyrad.assemble.utils.IPyradError: \n",
      "    Error: Params file already exists: params-ruber_denovo.txt\n",
      "    Use force argument to overwrite.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!ipyrad -n ruber_denovo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed742b9-61b6-44b1-b564-df0d16181796",
   "metadata": {},
   "source": [
    "This will create a params file with the defaults that ipyrad uses, we can modify these as we need . Whatever comes after the -n is what the assembly will be named\n",
    "\n",
    "Let’s go look at and edit that.\n",
    "\n",
    "## add in images of editing file, etc.\n",
    "\n",
    "\n",
    "We’ll change a few of these parameters:\n",
    "\n",
    "- [1]: This is where output will do, edit this to `/home/jupyter/RADseq_cloud_learn/ipyrad_out`\n",
    "\n",
    "- [2]: this needs to reflect the path to the `all_ruber.fastq.gz` file, which is: `/home/jupyter/RADseq_cloud_learn/radseq_cloud/ruber_data/all_ruber.fastq.gz`\n",
    "\n",
    "- [3]: this needs to be the path to `barcodes_samples.txt`: `/home/jupyter/RADseq_cloud_learn/radseq_cloud/ruber_data/barcodes_samples.txt`\n",
    "\n",
    "- [7]: dataype should be `ddrad`\n",
    "\n",
    "- [8]: restriction overhang is: `TGCAGG, GATC` these are the overhangs created by the restriction enzymes for ddRAD that was used for these data. I find these to be a pain to figure out, this is covered in the ipyrad params documentation\n",
    "\n",
    "- [14]: This is the clustering threshold for clustering reads into loci within samples. This is an important paramater that can have large effects on your final dataset. The default of `0.85` is good for phylogenetic datasets, but for population genetics, you will often want to use a higher threshold like 0.9 or 0.93. Let's use `0.9` here.\n",
    "\n",
    "- [27]: change to `*`, this will generate all output formats that ipyrad is currently capable of\n",
    "\n",
    "The rest of these are at generally reasonable values, although depending on your data, you may want to modify some of these. The parameters are all well documented [here](https://ipyrad.readthedocs.io/en/latest/6-params.html).\n",
    "\n",
    "For our final dataset, we'll want to set parameter [21] \"min_sample per locus\" to something higher to end up with a reasonable amount of missing data, but we'll deal with this later.\n",
    "\n",
    "We'll start by running steps 1-5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8507e3d-f742-4630-8014-c2938474b492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------------------------------------------------\n",
      "  ipyrad [v.0.9.104]\n",
      "  Interactive assembly and analysis of RAD-seq data\n",
      " ------------------------------------------------------------- \n",
      "  Parallel connection | fresh-noncontainer: 16 cores\n",
      "  \n",
      "  Step 1: Demultiplexing fastq data to Samples\n",
      "\n",
      "  Encountered an Error.\n",
      "  Message:     Error: Step 1 requires that you enter one of the following:\n",
      "        (1) a sorted_fastq_path\n",
      "        (2) a raw_fastq_path + barcodes_path\n",
      "    \n",
      "^C\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Run ipyrad with those parameters for steps 1-5 and using 16 cores\n",
    "!ipyrad -p params-ruber_denovo.txt -s 12345 -c 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b250661-b1d8-4e68-ae3e-c8da8fc4723e",
   "metadata": {},
   "source": [
    "This should take around 30 minutes. While that's running, familiarize yourself with the steps in ipyrad, which are thoroughly documented [here](https://ipyrad.readthedocs.io/en/master/7-outline.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a607b832-f2a6-4a78-9a60-655f137dba52",
   "metadata": {},
   "source": [
    "## Branching an assembly\n",
    "\n",
    "We only ran steps 1-5 above because the Fastq file that we started with includes mostly individuals of the red diamond rattlesnake, *Crotalus ruber*, but also a few individuals of other rattlesnake species to serve as outgrpoups. Right now, we want to make a dataset that includes only *C. ruber* individuals that we can run some popgen analyses on in the next session.\n",
    "\n",
    "ipyrad includes functionality to make new “branches” of the assembly using different parameters and/or including/excluding different individuals, and we’ll take advantage of that functionality here.\n",
    "\n",
    "- If we wanted to include all samples in the same dataset, we could've just run all 7 steps at once.\n",
    "\n",
    "To create a new branch with only the desired individuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f143598-1bf3-402e-b637-87bc6a3ef0a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipyrad.assemble.utils.IPyradError: \n",
      "            Could not find saved Assembly file (.json) in expected location.\n",
      "            Checks in: [project_dir]/[assembly_name].json\n",
      "            Checked: /home/jupyter/RADseq_cloud_learn/ruber_denovo.json\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# branch the assembly\n",
    "!ipyrad -p params-ruber_denovo.txt -b ruber_only_denovo radseq_cloud/ruber_data/names_ruber_all.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe72cd-1e3c-4a93-b6e7-8d6f2d49882f",
   "metadata": {},
   "source": [
    "This will use our old assembly and params file to generate a new branch, with params file `params-ruber_only_denovo.txt` that includes only samples in the `names_ruber_all.txt file`.\n",
    "\n",
    "We need to further edit this file to change parameter [21] “min_sample per locus”. The parameter defines how many how many individual samples a locus must have data for to include that locus in the final dataset. It controls the amount of missing data in the final dataset. Here, let's set this to `26` - this is about 75% of individuals and should result in a matrix that is ~75% or greater complete.\n",
    "\n",
    "Use your favorite text editor and make this change in the file params-ruber_only_denovo.txt:\n",
    "\n",
    "`26               ## [21] [min_samples_locus]: Min # samples per locus for output`\n",
    "\n",
    "Once that change has been made, run the final 2 steps in ipyrad. This should be very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556df84a-d2e6-4e40-9dae-64c81409535d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ipyrad -p params-ruber_only_denovo.txt -s 67 -c 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd6c2d-7431-47dd-b438-d6d3c834602a",
   "metadata": {},
   "source": [
    "## Examining the output\n",
    "\n",
    "\n",
    "Before you start analyzing your data, you should always take a look at the output stats.\n",
    "\n",
    "Take a look at the `ruber_only_denovo_stats.txt` file in the `ipyrad_out/ruber_only_denovo_outfiles` directory by opening it in Jupyterlab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa34a23-89ce-4d02-b40b-29d732ffdc1f",
   "metadata": {},
   "source": [
    "There should be about 2498 loci recovered in the assembly (last column of row `total_filtered_loci`). If we scroll down a bit in the table `The number of loci recovered for each Sample`, we can see that SD_Field_0506 has almost no loci shared with other samples, and SD_Field_1453 has only about half as many loci as most samples. We’ll want to remove these samples before moving on. \n",
    "\n",
    "- Note that SD_Field_0506 is an obviously failed sample, but for SD_Field_1453, you would likely want to try out some preliminary downstream analyses with and without this sample – I’ve already analyzed these data and decided it’s best to remove it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d973e6fa-16da-4f6b-9cc5-122ca373fdee",
   "metadata": {},
   "source": [
    "## Branch to remove low data samples\n",
    "\n",
    "Start by making a new names file to exclude SD_Field_0506 and SD_Field_1453 called `names_ruber_reduced.txt` and delete the lines containing `SD_Field_0506` and `SD_Field_1453`.\n",
    "\n",
    "\n",
    "Then do the branching and run step 7 on that new branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef23bd8-7fba-4ede-938b-2d9e373a92a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# branch\n",
    "!ipyrad -p params-ruber_only_denovo.txt -b ruber_reduced_denovo radseq_cloud/ruber_data/names_ruber_reduced.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7db89-4f08-49cb-bdc6-f5c93b15a642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ipyrad -p params-ruber_reduced_denovo.txt -s 7 -c 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16474d50-4c45-4c99-8fa8-31c975143f2b",
   "metadata": {},
   "source": [
    "Look at the stats for the new assembly in `ipyrad_out/ruber_reduced_denovo_outfiles/ruber_reduced_denovo_stats.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a054a01-aa43-458f-a8c8-d3bd7a8f8170",
   "metadata": {},
   "source": [
    "You should now see a slight decrease in the number of loci (I see 2451), but pretty good coverage across individuals, with no single sample having maassive amounts of missing data. This looks like a good dataset to move forward with.\n",
    "\n",
    "We have all sorts of variously formatted data files in the output directory. We'll upload the contents of that directory, as well as some of the metadata to a bucket so that we can use that data in the downstream analyses in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf340ddb-ef1c-4f52-9989-555d3f0b54ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new bucket\n",
    "! gsutil mb -l us-east4 gs://ruber-ipyrad-out\n",
    "\n",
    "# Copy the ipyrad output\n",
    "! gsutil -m cp /home/jupyter/RADseq_cloud_learn/ipyrad_out/ruber_reduced_denovo_outfiles/* gs://ruber-ipyrad-out/\n",
    "\n",
    "# Copy over the file of locality coordinates for each sample\n",
    "! gsutil cp /home/jupyter/RADseq_cloud_learn/radseq_cloud/ruber_data/Localities.csv gs://ruber-ipyrad-out/"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
