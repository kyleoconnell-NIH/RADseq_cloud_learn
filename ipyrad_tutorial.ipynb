{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7403033e-9b40-453a-8d04-43744a332db5",
   "metadata": {},
   "source": [
    "# Assembling RADseq data with ipyrad\n",
    "\n",
    "\n",
    "\n",
    "Restriction site-Associated DNA sequencing (RADseq) is a high-throughput genotyping technique used in molecular biology and genomics. It is one method to generate reduced representation libraries, which are methods that allow us to prepare and sequence hundreds to thousands of genomic regions from across the genome without sequencing the entire genome. RADseq methods use restriction enzymes to cut up the genome and then sequence DNA regions that are adjacent to these cut sites. The idea is that within the same species or relatively closely related species, restriction enzyme cut sites should mostly be at the same places and allow for the selection of shared loci across samples without needing to develop sepecific probes.\n",
    "\n",
    "\n",
    "One of the most commonly used RADseq approaches, and the one that we'll use here, is double digest RADseq (ddRADseq) which uses two enzymes to cut up the genome and then a size selection step to further reduce the total set of total set of loci, which should ideally result in fewer loci that require less sequencing effort and that overlap among samples.\n",
    "\n",
    "we’ll be working with empirical double digest RADseq data [(Peterson et al. 2012)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0037135) that I (Sean Harrington) generated as part of my PhD research at San Diego State University. The data are for a species of rattlesnake, the red diamond rattlesnake (Crotalus ruber), that is distributed across the Baja California peninsula and into southern California. I was interested in identifying if there is any population structure in *C. ruber* and inferring what population genetic and environmental forces have resulted in any existing structure. The data are single-end reads generated on an Illumina hiSeq. My analyses of these data are published in [Harrington et al. 2018](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0037135).\n",
    "\n",
    "\n",
    "\n",
    "The dataset is reasonably small and we should be able to quickly process and analyze it.\n",
    "\n",
    "We will use [ipyrad](https://ipyrad.readthedocs.io/en/master/) to process and assemble the raw data into alignments. ipyrad is a flexible python-based pipeline for taking various types of restriction-site associated data, processing them, and generated aligned datasets.\n",
    "\n",
    "iPyRad is capable of generating datasets either by mapping your raw reads to a reference genome or using a de novo assembly method that does not require a reference. We will use the de novo method today.\n",
    "\n",
    "If you need help with ipyrad outside of this workshop for specific issues, you can always post [here](https://app.gitter.im/#/room/#dereneaton_ipyrad:gitter.im). The developers are very responsive to queries.\n",
    "\n",
    "ipyrad is certainly not the only option for assembling RADseq data. [Stacks](https://catchenlab.life.illinois.edu/stacks/) and [dDocent](https://www.ddocent.com//) are other popular options, or there are various ways to manually assemble or map RADseq data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009069f0-bf36-4723-b83e-1f75699d6017",
   "metadata": {},
   "source": [
    "Files and basic setup\n",
    "\n",
    "The files we will use are:\n",
    "\n",
    "- all_ruber.fastq\n",
    "- barcodes_samples.txt\n",
    "- names_ruber_all.txt\n",
    "\n",
    "## Stuff about handling those files on the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46ee98a-a4a8-4ec6-97de-c3468a53a164",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://radseq_cloud/ruber_data/Localities.csv...\n",
      "Copying gs://radseq_cloud/ruber_data/barcodes_samples.txt...                    \n",
      "Copying gs://radseq_cloud/ruber_data/all_ruber.fastq.gz...\n",
      "Copying gs://radseq_cloud/ruber_data/names_ruber_all.txt...                     \n",
      "Copying gs://radseq_cloud/ruber_data/names_ruber_reduced.txt...                 \n",
      "- [5/5 files][  1.4 GiB/  1.4 GiB] 100% Done  78.8 MiB/s ETA 00:00:00           \n",
      "Operation completed over 5 objects/1.4 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r gs://radseq_cloud/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990df80-df0a-4e4a-a738-4e2ea53c68a6",
   "metadata": {},
   "source": [
    "## fastq format\n",
    "\n",
    "Before we start doing anything with the data, it's worth seeing what the raw data look like. The standard format for all raw data for genomic sequences is fastq.\n",
    "\n",
    "Let's take a look at the first 8 lines of the fastq file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0784010f-23fc-4c4a-ad8e-ef83163a90df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@SRR6143937.sra.1 1 length=96\n",
      "TGATCGCTAANAGCAAATTGAGTCCCCTGCCCATCAGTTGATGATGTCATTGGTACTTTCTATTGTGTCAGGTCTTAACTTGCCATGTTTTTTTACTTTTATTA\n",
      "+SRR6143937.sra.1 1 length=96\n",
      "IIIIIIIIFD#24AFHJJJJJJIIJJJJJJJJJJJJJIJJJJJJJJJJJJJJJGHIJJJJJJJJJHIJJJJJ@FHIHIDHIHHHFHFFFFFFDDDDDCEDDDDA\n",
      "@SRR6143937.sra.2 2 length=96\n",
      "TGATCGCTTGNAGGGGGCGCATGAAGAGCGCAGGCACAGAGCAAGGCCCCGCCCTCCCCAGGGACTCATTGTGCAGTAACCGGATTGACTTCTCATGCACGCAG\n",
      "+SRR6143937.sra.2 2 length=96\n",
      "IIIIIIIIFF#22<DHIHJJJJJIJIJJJIJJJJHHHHFFFFFEECDDDDDDDDDDDDDDDDB<@BDDDEDEDDEDDD>CCDDBDDDDDDDDCDEEDDDDDDDB\n",
      "\n",
      "gzip: stdout: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!zcat radseq_cloud/ruber_data/all_ruber.fastq.gz | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4b9cf6-c9e6-4128-8a1b-205cc9ee5113",
   "metadata": {},
   "source": [
    "- note that these reads are gzipped (end in .gz) you cannot directly look at them with head but instead need to use zcat, which reads gzipped files, and pipe the output to head. Fastqs are typically gzipped to save disk space and most genomics programs can read gzipped fastqs\n",
    "\n",
    "Each read from the sequencer is represented by 4 lines: the first 4 lines are the first read, the second set of 4 lines are the second read, etc. For each read, the first line is the header, and always starts with @. This contains a sequence identifier and various information about the read, often including information about the sequencing run. The second line, after the header, is the actual DNA sequence of the read. The next line always starts with + and may contain either no additional text, or the sequence identifier and extra information, as in the header. Line 4 for each read, following the + line, indicates the quality score for each DNA base in the read. This line will be exactly the same length as the DNA sequence in the second line, with e.g., the 4th character in this line corresponding to the quality of the 4th base in the sequence, etc.\n",
    "\n",
    "### INSTALL IPYRAD, EDIT LATER\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf95e68a-125b-42cf-bb9d-3a019ff3840e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 86.0M  100 86.0M    0     0   119M      0 --:--:-- --:--:-- --:--:--  119M\n",
      "ERROR: File or directory already exists: '/home/jupyter/mambaforge'\n",
      "If you want to update an existing installation, use the -u option.\n"
     ]
    }
   ],
   "source": [
    "! curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\n",
    "! bash Mambaforge-$(uname)-$(uname -m).sh -b -p $HOME/mambaforge 1>/dev/null\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e04e539-0d7b-4a67-8f06-eee1df3e486c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#add to your path\n",
    "import os \n",
    "os.environ[\"PATH\"] += os.pathsep + os.environ[\"HOME\"]+\"/mambaforge/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64498007-0e65-4c75-96cc-08b116b30717",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for: ['ipyrad']\n",
      "\n",
      "conda-forge/linux-64                                        Using cache\n",
      "conda-forge/noarch                                          Using cache\n",
      "bioconda/linux-64                                           Using cache\n",
      "bioconda/noarch                                             Using cache\n",
      "\n",
      "Pinned packages:\n",
      "  - python 3.10.*\n",
      "\n",
      "\n",
      "Transaction\n",
      "\n",
      "  Prefix: /opt/conda\n",
      "\n",
      "  All requested packages already installed\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! mamba install ipyrad -c conda-forge -c bioconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8f77058-2fcc-4e3e-8376-b440c1dc2117",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psutil==6.1.0\n",
      "  Downloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Downloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Installing collected packages: psutil\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.3\n",
      "    Uninstalling psutil-5.9.3:\n",
      "      Successfully uninstalled psutil-5.9.3\n",
      "Successfully installed psutil-6.1.0\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install psutil==6.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428fbb7-8628-4fee-8434-41c2f949fc5f",
   "metadata": {},
   "source": [
    "## Running iPyRad\n",
    "\n",
    "\n",
    "First, we need to generate a params file that contains the parameters we need to specify for ipyrad. In your scripts directory, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36d27bc2-55db-474f-97e4-25782a6a836c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/ipyrad\", line 6, in <module>\n",
      "    from ipyrad.__main__ import main\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipyrad/__init__.py\", line 4, in <module>\n",
      "    from .core.assembly import Assembly, merge\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipyrad/core/assembly.py\", line 22, in <module>\n",
      "    from ipyrad.core.Parallel import Parallel\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipyrad/core/Parallel.py\", line 20, in <module>\n",
      "    import ipyparallel as ipp\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipyparallel/__init__.py\", line 19, in <module>\n",
      "    from .cluster import Cluster  # noqa\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipyparallel/cluster/__init__.py\", line 1, in <module>\n",
      "    from .cluster import *  # noqa\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipyparallel/cluster/cluster.py\", line 76, in <module>\n",
      "    class Cluster(AsyncFirst, LoggingConfigurable):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipyparallel/cluster/cluster.py\", line 179, in Cluster\n",
      "    controller_launcher_class = Launcher(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipyparallel/traitlets.py\", line 23, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py\", line 2126, in __init__\n",
      "    super().__init__(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py\", line 536, in __init__\n",
      "    if self.help:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipyparallel/traitlets.py\", line 32, in help\n",
      "    for key, entry_point in self.load_entry_points().items():\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipyparallel/traitlets.py\", line 44, in load_entry_points\n",
      "    for entry_point in entry_points(group=self.entry_point_group)\n",
      "  File \"/opt/conda/lib/python3.10/importlib/metadata/__init__.py\", line 1021, in entry_points\n",
      "    return SelectableGroups.load(eps).select(**params)\n",
      "  File \"/opt/conda/lib/python3.10/importlib/metadata/__init__.py\", line 459, in load\n",
      "    ordered = sorted(eps, key=by_group)\n",
      "  File \"/opt/conda/lib/python3.10/importlib/metadata/__init__.py\", line 1018, in <genexpr>\n",
      "    eps = itertools.chain.from_iterable(\n",
      "  File \"/opt/conda/lib/python3.10/importlib/metadata/_itertools.py\", line 16, in unique_everseen\n",
      "    k = key(element)\n",
      "  File \"/opt/conda/lib/python3.10/importlib/metadata/__init__.py\", line 942, in _normalized_name\n",
      "    pass_none(Prepared.normalize)(self._name_from_stem(stem))\n",
      "  File \"/opt/conda/lib/python3.10/importlib/metadata/_functools.py\", line 99, in pass_none\n",
      "    @functools.wraps(func)\n",
      "  File \"/opt/conda/lib/python3.10/functools.py\", line 76, in wraps\n",
      "    return partial(update_wrapper, wrapped=wrapped,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!ipyrad -n ruber_denovo  # commented because I've already edited, don't want to overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed742b9-61b6-44b1-b564-df0d16181796",
   "metadata": {},
   "source": [
    "This will create a params file with the defaults that ipyrad uses, we can modify these as we need . Whatever comes after the -n is what the assembly will be named\n",
    "\n",
    "Let’s go look at and edit that.\n",
    "\n",
    "\n",
    "### best way to edit a text file on gcp?\n",
    "## will need to edit paths in here\n",
    "\n",
    "We’ll change a few of these parameters:\n",
    "\n",
    "- [1]: This is where output will do, edit this to `/home/jupyter/RADseq_cloud_learn/ipyrad_out`\n",
    "\n",
    "- [2]: this needs to reflect the path to the `all_ruber.fastq.gz` file, which is: `/home/jupyter/RADseq_cloud_learn/radseq_cloud/ruber_data/all_ruber.fastq.gz`\n",
    "\n",
    "- [3]: this needs to be the path to `barcodes_samples.txt`: `/home/jupyter/RADseq_cloud_learn/radseq_cloud/ruber_data/barcodes_samples.txt`\n",
    "\n",
    "- [7]: dataype should be `ddrad`\n",
    "\n",
    "- [8]: restriction overhang is: `TGCAGG, GATC` these are the overhangs created by the restriction enzymes for ddRAD that was used for these data. I find these to be a pain to figure out, this is covered in the ipyrad params documentation\n",
    "\n",
    "- [14]: This is the clustering threshold for clustering reads into loci within samples. This is an important paramater that can have large effects on your final dataset. The default of `0.85` is good for phylogenetic datasets, but for population genetics, you will often want to use a higher threshold like 0.9 or 0.93. Let's use `0.9` here.\n",
    "\n",
    "- [27]: change to `*`, this will generate all output formats that ipyrad is currently capable of\n",
    "\n",
    "The rest of these are at generally reasonable values, although depending on your data, you may want to modify some of these. The parameters are all well documented [here](https://ipyrad.readthedocs.io/en/latest/6-params.html).\n",
    "\n",
    "For our final dataset, we'll want to set parameter [21] \"min_sample per locus\" to something higher to end up with a reasonable amount of missing data, but we'll deal with this later.\n",
    "\n",
    "We'll start by running steps 1-5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8507e3d-f742-4630-8014-c2938474b492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------------------------------------------------\n",
      "  ipyrad [v.0.9.102]\n",
      "  Interactive assembly and analysis of RAD-seq data\n",
      " ------------------------------------------------------------- \n",
      "  Parallel connection | ruber-jup-test: 8 cores\n",
      "  \n",
      "  Step 1: Demultiplexing fastq data to Samples\n",
      "  [####################] 100% 0:03:26 | sorting reads          \n",
      "  [####################] 100% 0:01:28 | writing/compressing    \n",
      "  \n",
      "  Step 2: Filtering and trimming reads\n",
      "  [####################] 100% 0:00:56 | processing reads     \n",
      "  \n",
      "  Step 3: Clustering/Mapping reads within samples\n",
      "  [####################] 100% 0:00:10 | dereplicating          \n",
      "  [####################] 100% 0:03:50 | clustering/mapping     \n",
      "  [####################] 100% 0:00:00 | building clusters      \n",
      "  [####################] 100% 0:00:00 | chunking clusters      \n",
      "  [####################] 100% 0:11:06 | aligning clusters      \n",
      "  [####################] 100% 0:00:18 | concat clusters        \n",
      "  [####################] 100% 0:00:02 | calc cluster stats     \n",
      "  \n",
      "  Step 4: Joint estimation of error rate and heterozygosity\n",
      "  [####################] 100% 0:01:31 | inferring [H, E]       \n",
      "  \n",
      "  Step 5: Consensus base/allele calling \n",
      "  Mean error  [0.00165 sd=0.00092]\n",
      "  Mean hetero [0.01052 sd=0.00152]\n",
      "  [####################] 100% 0:00:02 | calculating depths     \n",
      "  [####################] 100% 0:00:02 | chunking clusters      \n",
      "  [####################] 100% 0:06:09 | consens calling        \n",
      "  [####################] 100% 0:00:08 | indexing alleles       \n",
      "  Parallel connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Run ipyrad with those parameters for steps 1-5 and using 16 cores\n",
    "!ipyrad -p params-ruber_denovo.txt -s 12345 -c 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b250661-b1d8-4e68-ae3e-c8da8fc4723e",
   "metadata": {},
   "source": [
    "This should take around 30 minutes. While that's running, familiarize yourself with the steps in ipyrad, which are thoroughly documented [here](https://ipyrad.readthedocs.io/en/master/7-outline.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a607b832-f2a6-4a78-9a60-655f137dba52",
   "metadata": {},
   "source": [
    "## Branching an assembly\n",
    "\n",
    "We only ran steps 1-5 above because the Fastq file that we started with includes mostly individuals of the red diamond rattlesnake, Crotalus ruber, but also a few outgroup taxa. Right now, we want to make a dataset that includes only C. ruber individuals that we can run some popgen analyses on in the next session.\n",
    "\n",
    "iPyRad includes functionality to make new “branches” of the assembly using different parameters and/or including/excluding different individuals, and we’ll take advantage of that functionality here.\n",
    "\n",
    "- If we wanted to include all samples in the same dataset, we could've just run all 7 steps at once.\n",
    "\n",
    "To create a new branch with only the desired individuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f143598-1bf3-402e-b637-87bc6a3ef0a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loading Assembly: ruber_denovo\n",
      "  from saved path: ~/RADseq_cloud_learn/ipyrad_out/ruber_denovo.json\n",
      "  creating a new branch called 'ruber_only_denovo' with 35 Samples\n",
      "  writing new params file to params-ruber_only_denovo.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# branch the assembly\n",
    "!ipyrad -p params-ruber_denovo.txt -b ruber_only_denovo radseq_cloud/ruber_data/names_ruber_all.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe72cd-1e3c-4a93-b6e7-8d6f2d49882f",
   "metadata": {},
   "source": [
    "This will use our old assembly and params file to generate a new branch, with params file `params-ruber_only_denovo.txt` that includes only samples in the `names_ruber_all.txt file`.\n",
    "\n",
    "We need to further edit this file to change parameter [21] “min_sample per locus”. The parameter defines how many how many individual samples a locus must have data for to include that locus in the final dataset. It controls the amount of missing data in the final dataset. Here, let's set this to `26` - this is about 75% of individuals and should result in a matrix that is ~75% or greater complete.\n",
    "\n",
    "Use your favorite text editor and make this change in the file params-ruber_only_denovo.txt:\n",
    "\n",
    "`26               ## [21] [min_samples_locus]: Min # samples per locus for output`\n",
    "\n",
    "Once that change has been made, run the final 2 steps in ipyrad. This should be very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "556df84a-d2e6-4e40-9dae-64c81409535d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loading Assembly: ruber_only_denovo\n",
      "  from saved path: ~/RADseq_cloud_learn/ipyrad_out/ruber_only_denovo.json\n",
      "\n",
      " -------------------------------------------------------------\n",
      "  ipyrad [v.0.9.102]\n",
      "  Interactive assembly and analysis of RAD-seq data\n",
      " ------------------------------------------------------------- \n",
      "  Parallel connection | ruber-jup-test: 16 cores\n",
      "  \n",
      "  Step 6: Clustering/Mapping across samples \n",
      "  [####################] 100% 0:00:03 | concatenating inputs   \n",
      "  [####################] 100% 0:00:31 | clustering across    \n",
      "  [####################] 100% 0:00:04 | building clusters      \n",
      "  [####################] 100% 0:01:09 | aligning clusters      \n",
      "  \n",
      "  Step 7: Filtering and formatting output files \n",
      "  [####################] 100% 0:00:16 | applying filters       \n",
      "  [####################] 100% 0:00:02 | building arrays        \n",
      "  [####################] 100% 0:00:02 | writing conversions    \n",
      "  [####################] 100% 0:00:01 | indexing vcf depths    \n",
      "  [####################] 100% 0:00:01 | writing vcf output     \n",
      "  Parallel connection closed.\n"
     ]
    }
   ],
   "source": [
    "!ipyrad -p params-ruber_only_denovo.txt -s 67 -c 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd6c2d-7431-47dd-b438-d6d3c834602a",
   "metadata": {},
   "source": [
    "## Examining the output\n",
    "\n",
    "\n",
    "Before you start analyzing your data, you should always take a look at the output stats.\n",
    "\n",
    "Take a look at the `ruber_only_denovo_stats.txt` file in the `ipyrad_out/ruber_only_denovo_outfiles` directory by opening it in Jupyterlab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa34a23-89ce-4d02-b40b-29d732ffdc1f",
   "metadata": {},
   "source": [
    "There should be about 2498 loci recovered in the assembly (last column of row `total_filtered_loci`). If we scroll down a bit in the table `The number of loci recovered for each Sample`, we can see that SD_Field_0506 has almost no loci shared with other samples, and SD_Field_1453 has only about half as many loci as most samples. We’ll want to remove these samples before moving on. Note that SD_Field_0506 is an obviously failed sample, but for SD_Field_1453, you would likely want to try out some preliminary downstream analyses with and without this sample – I’ve already analyzed these data and decided it’s best to remove it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d973e6fa-16da-4f6b-9cc5-122ca373fdee",
   "metadata": {},
   "source": [
    "## Branch to remove low data samples\n",
    "\n",
    "Start by making a new names file to exclude SD_Field_0506 and SD_Field_1453 called `names_ruber_reduced.txt` and delete the lines containing `SD_Field_0506` and `SD_Field_1453`.\n",
    "\n",
    "\n",
    "Then do the branching and run step 7 on that new branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ef23bd8-7fba-4ede-938b-2d9e373a92a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loading Assembly: ruber_only_denovo\n",
      "  from saved path: ~/RADseq_cloud_learn/ipyrad_out/ruber_only_denovo.json\n",
      "  creating a new branch called 'ruber_reduced_denovo' with 33 Samples\n",
      "  writing new params file to params-ruber_reduced_denovo.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# branch\n",
    "!ipyrad -p params-ruber_only_denovo.txt -b ruber_reduced_denovo radseq_cloud/ruber_data/names_ruber_reduced.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ead7db89-4f08-49cb-bdc6-f5c93b15a642",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loading Assembly: ruber_reduced_denovo\n",
      "  from saved path: ~/RADseq_cloud_learn/ipyrad_out/ruber_reduced_denovo.json\n",
      "\n",
      " -------------------------------------------------------------\n",
      "  ipyrad [v.0.9.102]\n",
      "  Interactive assembly and analysis of RAD-seq data\n",
      " ------------------------------------------------------------- \n",
      "  Parallel connection | ruber-jup-test: 16 cores\n",
      "  \n",
      "  Step 7: Filtering and formatting output files \n",
      "  [####################] 100% 0:00:19 | applying filters       \n",
      "  [####################] 100% 0:00:02 | building arrays        \n",
      "  [####################] 100% 0:00:02 | writing conversions    \n",
      "  [####################] 100% 0:00:01 | indexing vcf depths    \n",
      "  [####################] 100% 0:00:01 | writing vcf output     \n",
      "  Parallel connection closed.\n"
     ]
    }
   ],
   "source": [
    "!ipyrad -p params-ruber_reduced_denovo.txt -s 7 -c 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16474d50-4c45-4c99-8fa8-31c975143f2b",
   "metadata": {},
   "source": [
    "Look at the stats for the new assembly in `ipyrad_out/ruber_reduced_denovo_outfiles/ruber_reduced_denovo_stats.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a054a01-aa43-458f-a8c8-d3bd7a8f8170",
   "metadata": {},
   "source": [
    "You should now see a slight decrease in the number of loci (I see 2451), but pretty good coverage across individuals, with no single sample having maassive amounts of missing data. This looks like a good dataset to move forward with.\n",
    "\n",
    "We have all sorts of variously formatted data files in the output directory."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
